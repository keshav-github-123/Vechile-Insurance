
# ğŸš— Vehicle Insurance ML Project

An **end-to-end Machine Learning project** that demonstrates **MLOps best practices**, from data ingestion to model deployment, powered by **MongoDB, AWS, Docker, GitHub Actions, and CI/CD pipelines**.

This project aims to show recruiters and developers a **production-ready ML workflow** that is modular, scalable, and cloud-native.

---

## ğŸ“Œ Key Features

âœ… **Project Setup & Package Management**

* Custom project structure generated using `template.py`.
* Local packages handled via **`setup.py`** and **`pyproject.toml`**.
* Environment setup using **Conda** + `requirements.txt`.

âœ… **Database Integration**

* **MongoDB Atlas** used as the primary database.
* Dataset ingestion via Python notebooks into MongoDB.
* Data fetched as key-value and transformed into Pandas DataFrames.

âœ… **Logging & Exception Handling**

* Centralized **logging module** for better debugging.
* Custom **exception handling module** tested with `demo.py`.

âœ… **Data Pipeline Components**

* **Data Ingestion** â†’ Pulls raw data from MongoDB.
* **Data Validation** â†’ Schema-based checks with `schema.yaml`.
* **Data Transformation** â†’ Feature engineering & preprocessing.
* **Model Training** â†’ ML model building & evaluation.
* **Model Evaluation** â†’ Monitors model drift.
* **Model Pusher** â†’ Pushes model to AWS S3 registry.

âœ… **Cloud & AWS Services**

* **IAM** setup for secure AWS access.
* **S3 Bucket** for model storage & registry.
* Model versioning with threshold-based evaluation.

âœ… **CI/CD Pipeline**

* **Dockerized application** with `Dockerfile` & `.dockerignore`.
* **GitHub Actions Workflow** for automation.
* **ECR (Elastic Container Registry)** for image storage.
* **EC2 Deployment** with self-hosted GitHub runner.

âœ… **Web Application**

* Flask-based `app.py` with REST endpoints.
* `/train` â†’ Trigger model training.
* `/predict` â†’ Make predictions.
* Hosted on **EC2 public IP + port 5000**.

---

## ğŸ› ï¸ Tech Stack

* **Languages:** Python 3.10
* **ML Libraries:** Pandas, Scikit-learn, NumPy
* **Database:** MongoDB Atlas
* **Cloud:** AWS (IAM, S3, EC2, ECR)
* **CI/CD:** GitHub Actions, Docker
* **Orchestration:** Modular ML pipeline with configuration-driven design
* **Logging & Monitoring:** Custom Python logging + exception handling

---

## âš™ï¸ Project Workflow

```mermaid
flowchart TD
    A[Data Source: MongoDB Atlas] --> B[Data Ingestion]
    B --> C[Data Validation]
    C --> D[Data Transformation]
    D --> E[Model Training]
    E --> F[Model Evaluation]
    F -->|Push Best Model| G[AWS S3 Model Registry]
    G --> H[Model Pusher]
    H --> I[Prediction Pipeline]
    I --> J[Flask Web App on EC2]
```

---

## ğŸš€ Setup & Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone <repo_url>
cd Vehicle-Insurance-ML
```

### 2ï¸âƒ£ Setup Environment

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```

### 3ï¸âƒ£ MongoDB Setup

* Create free cluster on [MongoDB Atlas](https://www.mongodb.com/cloud/atlas).
* Setup **network access** â†’ `0.0.0.0/0`.
* Copy **connection string** & save as `MONGODB_URL` environment variable.

```bash
# Linux / Mac
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster-url"

# Windows PowerShell
$env:MONGODB_URL="mongodb+srv://<username>:<password>@cluster-url"
```

### 4ï¸âƒ£ Run Demo

```bash
python demo.py
```

---

## â˜ï¸ AWS Deployment

* **IAM User** with `AdministratorAccess`.
* **S3 Bucket** for model registry.
* **ECR Repo** for Docker images`.
* **EC2 Instance** (Ubuntu 24.04, T2 Medium).

### GitHub Actions Secrets:

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_DEFAULT_REGION`
* `ECR_REPO`

---

## ğŸ–¥ï¸ CI/CD Pipeline

1. Code pushed to GitHub â†’ Triggers workflow.
2. GitHub Action builds Docker image â†’ Pushes to **AWS ECR**.
3. EC2 Self-Hosted Runner pulls latest image.
4. Flask app hosted on **EC2 Public IP:5000**.

---

## ğŸ“‚ Project Structure

```
Vehicle-Insurance-ML/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ components/           # Data ingestion, validation, transformation, trainer
â”‚   â”œâ”€â”€ configuration/        # MongoDB, AWS, schema, pipeline configs
â”‚   â”œâ”€â”€ entity/               # Entities: config, artifact, estimator
â”‚   â”œâ”€â”€ pipeline/             # Training & prediction pipelines
â”‚   â”œâ”€â”€ utils/                # Helper utilities
â”‚   â””â”€â”€ logger, exception     # Logging & error handling
â”‚
â”‚â”€â”€ notebook/                 # EDA, MongoDB data push
â”‚â”€â”€ static/, template/        # Flask web app UI
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ setup.py
â”‚â”€â”€ pyproject.toml
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ .github/workflows/aws.yaml
â”‚â”€â”€ app.py
â”‚â”€â”€ demo.py
```

---

## ğŸŒŸ Highlight

This project is not just a **machine learning model**, it is a **complete MLOps workflow** demonstrating:

* How to move from **data ingestion** â†’ **model training** â†’ **deployment**.
* How to use **cloud-native CI/CD pipelines** for automation.
* How to serve ML predictions through a **production-ready web app**.

---